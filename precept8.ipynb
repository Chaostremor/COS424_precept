{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precept Week 8\n",
    "We cover three seperate topics.\n",
    "### Principal Component Analysis (PCA)\n",
    "### Optimization Methods\n",
    "### Hidden Markov Model (HMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#author Tianju Xue, 04/03/2019\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 1: Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is PCA?\n",
    "\n",
    "PCA is a classical technique for finding __low-dimensional representations__ which are linear projections of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the challenges/goals? \n",
    "\n",
    "Assume we have $N$ data $\\lbrace \\mathbf{x_n} \\rbrace^{N}_{n=1}$, where $\\mathbf{x_n} \\in \\mathbb{R}^D$. Our goal is to find another set of vectors $\\lbrace \\mathbf{y_n} \\rbrace^{N}_{n=1}$ where $\\mathbf{y_n} \\in \\mathbb{R}^K$ ($K<D$) that represents $\\mathbf{x_n}$ with minimum information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have $N=200$ meaning that we have 200 data. We have $D=2$ meaning that each $x_n$ is a two dimensional vector.\n",
    "\n",
    "We use __orthogonal decomposition__ to represent each $\\mathbf{x_n}$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{x_n} \\approx \\widehat{\\mathbf{x_n}} = \\underbrace{\\dfrac{\\mathbf{x_n}\\cdot \\mathbf{u_1} }{\\mathbf{u_1}\\cdot\\mathbf{u_1}}}_{\\mathbf{y_n}(1)} \\mathbf{u_1} + \\underbrace{\\dfrac{\\mathbf{x_n}\\cdot \\mathbf{u_2} }{\\mathbf{u_2}\\cdot \\mathbf{u_2}}}_{\\mathbf{y_n}(2)} \\mathbf{u_2}+ ... + \\underbrace{\\dfrac{\\mathbf{x_n}\\cdot \\mathbf{u_K} }{\\mathbf{u_K}\\cdot \\mathbf{u_K}}}_{\\mathbf{y_n}(K)}\\mathbf{u_K}\n",
    "\\end{align}\n",
    "\n",
    "As long as the set of orthogonal basis $\\lbrace\\mathbf{u_n}\\rbrace^{K}_{n=1}$ is found, we are able to get  $\\lbrace\\mathbf{y_n}\\rbrace^{N}_{n=1}$ simply by inspecting the coefficients indicated above. \n",
    "\n",
    "For more details (like the criteria of selecting the basis set) you can refer to lecture notes or reference [2] listed in the end of this material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $n\\_compnents=2$ means $K=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get $\\mathbf{u_1} = (-0.94446029, -0.32862557)$ and $\\mathbf{u_2} = (-0.32862557, 0.94446029)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize $\\mathbf{u_1}$ and $\\mathbf{u_2}$. Notice that since $D=K=2$, we have an exact representation of $\\lbrace \\mathbf{x_n} \\rbrace^{N}_{n=1}$ using $\\lbrace \\mathbf{y_n} \\rbrace^{N}_{n=1}$ in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape (N, D):   \", X.shape)\n",
    "print(\"transformed shape (N, K):\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change $K=2$ to $K=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
    "plt.legend([r'$\\mathbf{x_n}$', r'$\\widehat{\\mathbf{x_n}}$'])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform data reconstruction. We construct $\\widehat{\\mathbf{x_n}}$ using $\\mathbf{y_n}$ and $\\lbrace\\mathbf{u_n}\\rbrace^{K}_{n=1}$. How much infomation do we lose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 2: Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is gradient descent?\n",
    "\n",
    "Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function $f$ with respect to its argument $w$. \n",
    "\n",
    "The general form looks like: $w^{n+1} = w^{n} - \\alpha \\nabla_{w^n} f $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several things to notice:\n",
    "1. $f(w)$ should be differentiable, or the derivative is not too hard to compute.\n",
    "2. It can only find a local minimum. However, for convex functions, local minimum implies global minimum.\n",
    "3. In many cases, $f$ represents loss function while $w$ represents weights.\n",
    "4. Automatic differentiation makes gradient descent super powerful in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression example:\n",
    "\n",
    "Given the loss function:\n",
    "\n",
    "$f(m,b) =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2$\n",
    "\n",
    "The gradient can be calculated as:\n",
    "\n",
    "$\\begin{split}\\nabla_{(m, b)} f =\n",
    "   \\begin{bmatrix}\n",
    "     \\frac{df}{dm}\\\\\n",
    "     \\frac{df}{db}\\\\\n",
    "    \\end{bmatrix}\n",
    "=\n",
    "   \\begin{bmatrix}\n",
    "     \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\\\\n",
    "     \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\\\\n",
    "    \\end{bmatrix}\\end{split}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of key step in gradient descent algorithm\n",
    "def update_weights(m, b, X, Y, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # Calculate partial derivatives\n",
    "        # -2x(y - (mx + b))\n",
    "        m_deriv += -2*X[i] * (Y[i] - (m*X[i] + b))\n",
    "\n",
    "        # -2(y - (mx + b))\n",
    "        b_deriv += -2*(Y[i] - (m*X[i] + b))\n",
    "\n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Expectation Maximization (EM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is EM?\n",
    "\n",
    "An expectationâ€“maximization (EM) algorithmis an iterative method to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General idea:\n",
    "\n",
    "Given the statistical model which generates a set $\\mathbf {X}$ of \n",
    "observed data, a set of unobserved latent data or missing values $\\mathbf {Z}$ , and a vector of unknown parameters   $\\boldsymbol {\\theta }$, the maximum log likelihood estimate (MLE) of the unknown parameters is determined by maximizing the marginal likelihood of the observed data \n",
    "\n",
    "\\begin{align}\n",
    "L({\\boldsymbol {\\theta }})=\\textrm{log}\\big( P(\\mathbf {X} |{\\boldsymbol {\\theta }})\\Big)= \\textrm{log}\\big( \\sum_{\\mathbf{Z}} P(\\mathbf {X} | \\mathbf {Z}, \\boldsymbol{\\theta}) P( \\mathbf {Z} | \\boldsymbol{\\theta} )  \\big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 3: Hidden Markov Model (HMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is HMM?\n",
    "\n",
    "The HMM is a generative probabilistic model, in which a sequence of observable\n",
    "$\\mathbf{X}$ variables is generated by a sequence of internal hidden\n",
    "states $\\mathbf{Z}$. The hidden states are not observed directly.\n",
    "The transitions between hidden states are assumed to have the form of a\n",
    "(first-order) Markov chain. They can be specified by the start probability\n",
    "vector $\\boldsymbol{\\pi}$ and a transition probability matrix\n",
    "$\\mathbf{A}$. The emission probability of an observable can be any\n",
    "distribution with parameters $\\boldsymbol{\\theta}$ conditioned on the\n",
    "current hidden state. The HMM is completely determined by the parameter set\n",
    "$\\boldsymbol{\\lambda} = (\\boldsymbol{\\pi}, \\mathbf{A}, \\boldsymbol{\\theta})$.\n",
    "\n",
    "What are the three fundamental problems for HMMs?\n",
    "\n",
    "* Given the model parameters and observed data, calculate the likelihood\n",
    "  of the data $P(\\mathbf{X}|\\boldsymbol{\\lambda})$.\n",
    "  \n",
    "* Given just the observed data $\\mathbf{X}$, estimate the model parameters $\\boldsymbol{\\lambda}$.\n",
    "\n",
    "* Given the model parameters $\\boldsymbol{\\lambda}$ and observed data $\\mathbf{X}$, estimate the optimal\n",
    "  sequence of hidden states $\\mathbf{Z}$ where $P(\\mathbf{Z}|\\mathbf{X})$ is maximized.\n",
    "\n",
    "What are the solutions?\n",
    "\n",
    "* The first problem can be solved by Forward-Backward algorithm.\n",
    "* The second problem can be solved by an iterative Expectation-Maximization (EM) algorithm, known as the Baum-Welch algorithm.\n",
    "* The third problem can be solved by dynamic programming algorithms known as the Viterbi algorithm.\n",
    "\n",
    "Sounds familiar? Recall the procedures of doing Naive Bayes classifier and Gaussian Mixture Models, can you find any similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: The sklearn.hmm module has been deprecated due to it no longer matching the scope and the API of the project. It becomes an independent python package now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startprob = np.array([0.6, 0.3, 0.1, 0.0])\n",
    "# The transition matrix, note that there are no transitions possible\n",
    "# between component 1 and 3\n",
    "transmat = np.array([[0.7, 0.2, 0.0, 0.1],\n",
    "                     [0.3, 0.5, 0.2, 0.0],\n",
    "                     [0.0, 0.3, 0.5, 0.2],\n",
    "                     [0.2, 0.0, 0.2, 0.6]])\n",
    "# The means of each component\n",
    "means = np.array([[0.0,  0.0],\n",
    "                  [0.0, 11.0],\n",
    "                  [9.0, 10.0],\n",
    "                  [11.0, -1.0]])\n",
    "# The covariance of each component\n",
    "covars = .5 * np.tile(np.identity(2), (4, 1, 1))\n",
    "\n",
    "# Build an HMM instance and set parameters\n",
    "model = hmm.GaussianHMM(n_components=4, covariance_type=\"full\")\n",
    "\n",
    "# Instead of fitting it from the data, we directly set the estimated\n",
    "# parameters, the means and covariance of the components\n",
    "model.startprob_ = startprob\n",
    "model.transmat_ = transmat\n",
    "model.means_ = means\n",
    "model.covars_ = covars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is $\\boldsymbol{\\lambda} = (\\boldsymbol{\\pi}, \\mathbf{A}, \\boldsymbol{\\theta})$ in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "X, Z = model.sample(500)\n",
    "\n",
    "# Plot the sampled data\n",
    "plt.plot(X[:, 0], X[:, 1], \".-\", label=\"observations\", ms=6,\n",
    "         mfc=\"orange\", alpha=0.7)\n",
    "\n",
    "# Indicate the component numbers\n",
    "for i, m in enumerate(means):\n",
    "    plt.text(m[0], m[1], 'Component %i' % (i + 1),\n",
    "             size=17, horizontalalignment='center',\n",
    "             bbox=dict(alpha=.7, facecolor='w'))\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain why there is no transition between component 1 and component 3 (or component 2 and component 4)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refereces:\n",
    "\n",
    "[1] https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "\n",
    "[2] https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/pca.pdf\n",
    "\n",
    "[3] https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html\n",
    "\n",
    "[4] https://github.com/hmmlearn/hmmlearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
